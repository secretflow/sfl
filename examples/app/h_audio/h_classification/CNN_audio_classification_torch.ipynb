{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 PyTorch  在 SecretFlow 中实现水平联邦音频分类任务\n",
    "\n",
    "## 引言\n",
    "本教程基于 PyTorch 的 [微软教程：使用 PyTorch 进行音频分类简介教程](https://learn.microsoft.com/zh-cn/training/modules/intro-audio-classification-pytorch/) 而改写，通过本教程，您将了解到现有的基于 PyTorch 的示例如何快速地迁移到 SecretFlow 隐语的联邦学习框架之下，实现模型的联邦学习化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单机模式\n",
    "\n",
    "### 小节引言\n",
    "\n",
    "本小节的代码主要来自于 [使用 PyTorch 进行音频分类简介教程](https://learn.microsoft.com/zh-cn/training/modules/intro-audio-classification-pytorch/) ，主要讲解如何在 PyTorch 下利用简单的神经网络进行音频分类。 为了教程的简洁，本小节仅仅简要介绍了一下各部分的功能；对于实现的具体解析，请读者移步参考[原教程](https://learn.microsoft.com/zh-cn/training/modules/intro-audio-classification-pytorch/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装 TorchAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/audio-pytorch/install-packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "import os\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载 speech commands 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dir = os.getcwd()\n",
    "folder = 'data'\n",
    "print(f'Data directory will be: {default_dir}/{folder}')\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data folder exists.\")\n",
    "else:\n",
    "    print(\"Creating folder.\")\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_speechcommands = torchaudio.datasets.SPEECHCOMMANDS(\n",
    "    f'./{folder}/', download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'./{folder}/SpeechCommands/speech_commands_v0.02/')\n",
    "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
    "# back to default directory\n",
    "os.chdir(default_dir)\n",
    "print(f'Total Labels: {len(labels)} \\n')\n",
    "print(f'Label Names: {labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把声音转换为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/SpeechCommands/speech_commands_v0.02/yes/00f0204f_nohash_0.wav\"\n",
    "waveform, sample_rate = torchaudio.load(uri=filename, num_frames=3)\n",
    "print(f'waveform tensor with 3 frames:  {waveform} \\n')\n",
    "waveform, sample_rate = torchaudio.load(uri=filename, num_frames=3, frame_offset=2)\n",
    "print(f'waveform tensor with 2 frame_offsets: {waveform} \\n')\n",
    "waveform, sample_rate = torchaudio.load(uri=filename)\n",
    "print(f'waveform tensor:  {waveform}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化波形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio(filename):\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "    print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "    print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(waveform.t().numpy())\n",
    "\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/SpeechCommands/speech_commands_v0.02/yes/00f0204f_nohash_0.wav\"\n",
    "waveform, sample_rate = plot_audio(filename)\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/SpeechCommands/speech_commands_v0.02/no/0b40aa8e_nohash_0.wav\"\n",
    "waveform, sample_rate = plot_audio(filename)\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集文件夹进入 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(path: str, label: str):\n",
    "    dataset = []\n",
    "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)\n",
    "        speaker, _ = os.path.splitext(filename)\n",
    "        speaker_id, utterance_number = speaker.split(\"_nohash_\")\n",
    "        utterance_number = int(utterance_number)\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_speechcommands_yes = load_audio_files(\n",
    "    './data/SpeechCommands/speech_commands_v0.02/yes', 'yes'\n",
    ")\n",
    "trainset_speechcommands_no = load_audio_files(\n",
    "    './data/SpeechCommands/speech_commands_v0.02/no', 'no'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
    "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_yes = torch.utils.data.DataLoader(\n",
    "    trainset_speechcommands_yes, batch_size=1, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_no = torch.utils.data.DataLoader(\n",
    "    trainset_speechcommands_no, batch_size=1, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_waveform = trainset_speechcommands_yes[0][0]\n",
    "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
    "print(f'Yes Waveform: {yes_waveform}')\n",
    "print(f'Yes Sample Rate: {yes_sample_rate}')\n",
    "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
    "print(f'Yes ID: {trainset_speechcommands_yes[0][3]} \\n')\n",
    "\n",
    "no_waveform = trainset_speechcommands_no[0][0]\n",
    "no_sample_rate = trainset_speechcommands_no[0][1]\n",
    "print(f'No Waveform: {no_waveform}')\n",
    "print(f'No Sample Rate: {no_sample_rate}')\n",
    "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
    "print(f'No ID: {trainset_speechcommands_no[0][3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变换和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_waveform(waveform, sample_rate, label):\n",
    "    print(\n",
    "        \"Waveform: {}\\nSample rate: {}\\nLabels: {} \\n\".format(\n",
    "            waveform, sample_rate, label\n",
    "        )\n",
    "    )\n",
    "    new_sample_rate = sample_rate / 10\n",
    "\n",
    "    # Resample applies to a single channel, we resample first channel here\n",
    "    channel = 0\n",
    "    waveform_transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(\n",
    "        waveform[channel, :].view(1, -1)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Shape of transformed waveform: {}\\nSample rate: {}\".format(\n",
    "            waveform_transformed.size(), new_sample_rate\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(waveform_transformed[0, :].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(yes_waveform, yes_sample_rate, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(no_waveform, no_sample_rate, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 频谱图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(waveform_classA, waveform_classB):\n",
    "    yes_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classA)\n",
    "    print(\"\\nShape of yes spectrogram: {}\".format(yes_spectrogram.size()))\n",
    "\n",
    "    no_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classB)\n",
    "    print(\"Shape of no spectrogram: {}\".format(no_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Features of {}\".format('no'))\n",
    "    plt.imshow(yes_spectrogram.log2()[0, :, :].numpy(), cmap='viridis')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Features of {}\".format('yes'))\n",
    "    plt.imshow(no_spectrogram.log2()[0, :, :].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(yes_waveform, no_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梅尔频谱图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_melspectrogram(waveform, sample_rate):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mel_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(mel_spectrogram.log2()[0, :, :].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_melspectrogram(yes_waveform, yes_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梅尔倒频谱系数（Mel-frequency cepstral coefficients , MFCC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mfcc(waveform, sample_rate):\n",
    "    mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate=sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    fig1 = plt.gcf()\n",
    "    plt.imshow(mfcc_spectrogram.log2()[0, :, :].numpy(), cmap='viridis')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(mfcc_spectrogram.log2()[0, :, :].numpy())\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mfcc(no_waveform, no_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于频谱图创建图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_images(trainloader, label_dir):\n",
    "    # make directory\n",
    "    directory = f'./data/spectrograms/{label_dir}/'\n",
    "    if os.path.isdir(directory):\n",
    "        print(\"Data exists for\", label_dir)\n",
    "    else:\n",
    "        os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            waveform = data[0]\n",
    "            sample_rate = data[1][0]\n",
    "            label = data[2]\n",
    "            ID = data[3]\n",
    "\n",
    "            # create transformed waveforms\n",
    "            spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.imsave(\n",
    "                f'./data/spectrograms/{label_dir}/spec_img{i}.png',\n",
    "                spectrogram_tensor[0].log2()[0, :, :].numpy(),\n",
    "                cmap='viridis',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc_images(trainloader, label_dir):\n",
    "    # make directory\n",
    "    os.makedirs(f'./data/mfcc_spectrograms/{label_dir}/', mode=0o777, exist_ok=True)\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        waveform = data[0]\n",
    "        sample_rate = data[1][0]\n",
    "        label = data[2]\n",
    "        ID = data[3]\n",
    "\n",
    "        mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate=sample_rate)(waveform)\n",
    "\n",
    "        plt.figure()\n",
    "        fig1 = plt.gcf()\n",
    "        plt.imshow(mfcc_spectrogram[0].log2()[0, :, :].numpy(), cmap='viridis')\n",
    "        plt.draw()\n",
    "        fig1.savefig(f'./data/mfcc_spectrograms/{label_dir}/spec_img{i}.png', dpi=100)\n",
    "\n",
    "        # spectorgram_train.append([spectrogram_tensor, label, sample_rate, ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram_images(trainloader_yes, 'yes')\n",
    "create_spectrogram_images(trainloader_no, 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载频谱图到 DataLoader 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/spectrograms'  # looking in subfolder train\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201, 81)), transforms.ToTensor()]),\n",
    ")\n",
    "print(yes_no_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = yes_no_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练数据和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to test and train\n",
    "# use 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(\n",
    "    yes_no_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "print(\"Training size:\", len(yes_no_train_dataset))\n",
    "print(\"Testing size:\", len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset, batch_size=15, num_workers=2, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset, batch_size=15, num_workers=2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = train_dataloader.dataset[0][0][0][0]\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "class_map = ['no', 'yes']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, Y) in enumerate(test_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        pred = model(X)\n",
    "        print(\n",
    "            \"Predicted:\\nvalue={}, class_name= {}\\n\".format(\n",
    "                pred[0].argmax(0), class_map[pred[0].argmax(0)]\n",
    "            )\n",
    "        )\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(Y[0], class_map[Y[0]]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 联邦学习\n",
    "### 小节引言\n",
    "通过单机模式，我们基于微软的[教程](https://learn.microsoft.com/zh-cn/training/modules/intro-audio-classification-pytorch/) 完成了基于 PyTorch 的音频分类任务。接下来，我们将看到如何将单机模式下的音频分类任务迁移到隐语的联邦学习模式之下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据划分\n",
    "首先为了模拟联邦学习多方参与的场景设定，我们先人为进行一下数据集划分。为方便演示，我们对数据按参与方进行均匀划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看数据集结构\n",
    "我们可以看到数据集的路径是 **\"./data/spectrograms\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdataset_yes_path = os.path.join(data_path, 'yes')\n",
    "os.listdir(subdataset_yes_path)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给定参与方\n",
    "我们假定联邦学习的数据拥有方是 **alice** 和 **bob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_list = ['alice', 'bob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立数据文件夹\n",
    "\n",
    "由上述结果，我们可以看到，语音分类的训练数据集经过预处理后，主要由频谱图组成，并且包含两个种类：是（yes）和否（no），分别位于 **\"./data/spectrograms/yes\"** 和 **\"./data/spectrograms/no\"** ；联邦学习模型的数据输入从此处开始；我们的参与方数据文件夹分别为 **\"./fl-data/spectrograms/alice/\"** 和 **\"./fl-data/spectrograms/bob/\"**；所以我们分别在参与方数据文件夹下建立 **'yes'** 和 **'no'** 来保存对应的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"spectrograms\"\n",
    "parties_path_list = []\n",
    "split_dataset_path = os.path.join('.', 'fl-data', dataset_name)\n",
    "\n",
    "for party in parties_list:\n",
    "    party_path = os.path.join('.', 'fl-data', dataset_name, party)\n",
    "    os.makedirs(party_path, exist_ok=True)\n",
    "    parties_path_list.append(party_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'README.md' in commands:\n",
    "    commands.remove('README.md')\n",
    "elif '.DS_Store' in commands:\n",
    "    commands.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进行数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import shutil\n",
    "\n",
    "parties_num = len(parties_list)\n",
    "for command in commands:\n",
    "    command_path = join(data_path, command)\n",
    "    for party_path in parties_path_list:\n",
    "        party_command_path = join(party_path, command)\n",
    "        print(party_command_path)\n",
    "        os.makedirs(party_command_path, exist_ok=True)\n",
    "\n",
    "    index = 0\n",
    "    for wav_name in os.listdir(command_path):\n",
    "        wav_path = join(command_path, wav_name)\n",
    "        target_dir_path = join(\n",
    "            '.', 'fl-data', dataset_name, parties_list[index % parties_num], command\n",
    "        )\n",
    "        shutil.copy(wav_path, target_dir_path)\n",
    "        # if you want to watch the progress of copying the files, please uncomment the following line\n",
    "        # print(f'copy {wav_path}-->{target_dir_path}')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看参与方的各个文件夹下所拥有的文件数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for command in commands:\n",
    "    for party_path in parties_path_list:\n",
    "        command_path = join(party_path, command)\n",
    "        file_num = len(os.listdir(command_path))\n",
    "        print(f'{command_path} : {file_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐语环境初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "sf.init(['alice', 'bob', 'charlie'], address=\"local\", log_to_driver=False)\n",
    "alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装 DataBuilder\n",
    "\n",
    "在迁移过程，对于数据集的预处理方式，联邦学习模式和单机模式是一样的，我们不再重复。为了完成迁移适配过程，我们只需要参考[在 SecretFlow 中使用自定义 DataBuilder（Torch）](https://github.com/secretflow/secretflow/blob/main/docs/tutorial/CustomDataLoaderTorch.ipynb) 封装我们自定义 DataBuilder 即可。现在，参考原教程，我们封装对应的DataBuilder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_builder(\n",
    "    batch_size=32,\n",
    "    train_split=0.8,\n",
    "    shuffle=True,\n",
    "    random_seed=1234,\n",
    "):\n",
    "    def dataset_builder(data_path, stage=\"train\"):\n",
    "        \"\"\" \"\"\"\n",
    "        import math\n",
    "\n",
    "        import numpy as np\n",
    "        from torch.utils.data import DataLoader\n",
    "        from torch.utils.data.sampler import SubsetRandomSampler\n",
    "        from torchvision import datasets, transforms\n",
    "\n",
    "        # Define dataset\n",
    "        yes_no_dataset = datasets.ImageFolder(\n",
    "            root=data_path,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize((201, 81)), transforms.ToTensor()]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        dataset_size = len(yes_no_dataset)\n",
    "        # Define sampler\n",
    "\n",
    "        indices = list(range(dataset_size))\n",
    "        if shuffle:\n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(indices)\n",
    "        split = int(np.floor(train_split * dataset_size))\n",
    "        train_indices, val_indices = indices[:split], indices[split:]\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        # Define databuilder\n",
    "        train_loader = DataLoader(\n",
    "            yes_no_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            yes_no_dataset, batch_size=batch_size, sampler=valid_sampler\n",
    "        )\n",
    "\n",
    "        # Return\n",
    "        if stage == \"train\":\n",
    "            train_step_per_epoch = math.ceil(split / batch_size)\n",
    "\n",
    "            return train_loader, train_step_per_epoch\n",
    "        elif stage == \"eval\":\n",
    "            eval_step_per_epoch = math.ceil((dataset_size - split) / batch_size)\n",
    "            return valid_loader, eval_step_per_epoch\n",
    "\n",
    "    return dataset_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 dataset_builder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset dict\n",
    "data_builder_dict = {\n",
    "    alice: create_dataset_builder(\n",
    "        batch_size=32,\n",
    "        train_split=0.8,\n",
    "        shuffle=False,\n",
    "        random_seed=1234,\n",
    "    ),\n",
    "    bob: create_dataset_builder(\n",
    "        batch_size=32,\n",
    "        train_split=0.8,\n",
    "        shuffle=False,\n",
    "        random_seed=1234,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构\n",
    "得益于隐语优异的设计，我们只需要将单机模式下定义的网络结构，由原来的继承`torch.nn.Module`改为继承即可`sfl.ml.nn.core.torch.BaseModule`即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfl.ml.nn.core.torch import BaseModule\n",
    "\n",
    "\n",
    "class ConvNet(BaseModule):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfl.ml.nn import FLModel\n",
    "from secretflow.security.aggregation import SecureAggregator\n",
    "from torch import nn, optim\n",
    "from torchmetrics import Accuracy, Precision\n",
    "from sfl.ml.nn.core.torch import metric_wrapper, optim_wrapper, TorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 Torch 后端的 FLModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_list = [alice, bob]\n",
    "aggregator = SecureAggregator(charlie, [alice, bob])\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "# torch model\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "optim_fn = optim_wrapper(optim.Adam, lr=1e-3)\n",
    "\n",
    "model_def = TorchModel(\n",
    "    model_fn=ConvNet,\n",
    "    loss_fn=loss_fn,\n",
    "    optim_fn=optim_fn,\n",
    "    metrics=[\n",
    "        metric_wrapper(\n",
    "            Accuracy, task=\"multiclass\", num_classes=num_classes, average='micro'\n",
    "        ),\n",
    "        metric_wrapper(\n",
    "            Precision, task=\"multiclass\", num_classes=num_classes, average='micro'\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "fed_model = FLModel(\n",
    "    device_list=device_list,\n",
    "    model=model_def,\n",
    "    aggregator=aggregator,\n",
    "    backend=\"torch\",\n",
    "    strategy=\"fed_avg_w\",\n",
    "    random_seed=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给出参与方数据集路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    alice: parties_path_list[0],\n",
    "    bob: parties_path_list[1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练联邦学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fed_model.fit(\n",
    "    data,\n",
    "    None,\n",
    "    validation_data=data,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    aggregate_freq=2,\n",
    "    sampler_method=\"batch\",\n",
    "    random_seed=1234,\n",
    "    dp_spent_step_freq=1,\n",
    "    dataset_builder=data_builder_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化训练历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.global_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Draw accuracy values for training & validation\n",
    "plt.plot(history.global_history['multiclassaccuracy'])\n",
    "plt.plot(history.global_history['val_multiclassaccuracy'])\n",
    "plt.title('FLModel accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "可以看到，将单机模式下的音频分类任务，迁移到隐语的联邦学习模式，不仅非常的容易，而且不需要开发者继续编写训练函数和测试函数，充分展现了隐语的易用性和好用性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过本教程，您将学习到：\n",
    "- 如何基于 PyTorch 开展音频分类任务\n",
    "- 如何编写少量代码即可将单机模式下的音频分类模型迁移到隐语的联邦学习框架之下 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
